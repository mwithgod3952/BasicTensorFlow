{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow_summary_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOXMwUclv7KBPDn+T770L+I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mwithgod3952/BasicTensorFlow/blob/main/Tensorflow_summary_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    TensorFlow has two main components that we will try to understand.\n",
        "        * Graphs\n",
        "        * Sessions\n",
        "    TensorFlow works by building a graph of defined computations. It is simply a way of defining the operations.\n",
        "    A TensorFlow session allows parts of the graph to be executed. It allocates memory and resources and handles the execution of the operations and computations we've defined.  \n",
        "        If i were to create some variable that gets added to the graph, and maybe that variable is the sum or the summation of two other variables.\n",
        "        what we need to understand is that it doesn't actually evaluate. it simply states that is the computation we've defined. \n",
        "        we just have that equation there. We know that this is the value, but we haven't evaluated it.  \n",
        "\n",
        "    Swssion is essentially a way to execute part or entire graph. \n",
        "    So when we start in a session, what we do is we start executing different aspects of the graph.\n",
        "    So we start at the lowest level of the graph where nothing is dependent on anything else, and we move our way through the graph, and start doing all of the different partial computations that we've defined."
      ],
      "metadata": {
        "id": "5bfNc9ui75ZZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x5Kzy3plg-F",
        "outputId": "875077c8-5aaa-45ff-c910-cefc8cbdcdc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive; drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content/drive/MyDrive/class_folder/Basic_Tensorflow')\n",
        "if os.getcwd().split('/')[-1] == 'Basic_Tensorflow':\n",
        "    wp = os.getcwd()\n",
        "else:\n",
        "    print('error is occured')"
      ],
      "metadata": {
        "id": "VDObIDPM4yNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ***Importing TensorFlow***"
      ],
      "metadata": {
        "id": "zBkKw7oyXyup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    - The first steps here is going to be to select the correct version of TensorFlow"
      ],
      "metadata": {
        "id": "a7WaelwpXhTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(tf.version)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSaa8Rzs5h0l",
        "outputId": "18756cef-bb72-4e0f-9938-36ed9cbc808a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<module 'tensorflow._api.v2.version' from '/usr/local/lib/python3.7/dist-packages/tensorflow/_api/v2/version/__init__.py'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ***Tensors***"
      ],
      "metadata": {
        "id": "HAaIepzrY9jQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    A tensor is a generalization of vectors and matrices to potentially higher dimensions.\n",
        "    Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes."
      ],
      "metadata": {
        "id": "sEvoqN9mZEbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***matrix generation***"
      ],
      "metadata": {
        "id": "3kev6-TUenRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here is a \"scalar\" tensor . A scalar contains a single value, and no axes\n",
        "Rank0_Tensor = tf.constant(4)\n",
        "print(Rank0_Tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IracVkjEZu_X",
        "outputId": "6d9fac5f-69cc-425d-edb9-c402b958162e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(4, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A rank-1 tensor is like a list of values. A vector has one asxis/\n",
        "Rank1_Tensor = tf.constant([2.0, 3.0, 4.0])\n",
        "print(Rank1_Tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akleLgC-aK-f",
        "outputId": "ad491dcc-0ce4-4809-8bc0-9d1c41d8cfb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A matrix or rnak-2 tensor has two axes\n",
        "# The shape of this matrix is 3, 2\n",
        "Rank2_Tensor = tf.constant([[1, 2],\n",
        "                            [3, 4],\n",
        "                            [5, 6]], dtype=tf.float16)\n",
        "print(Rank2_Tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLhCOShKasKk",
        "outputId": "3b8ced47-d33d-479a-9d5c-4aa9ac3e6ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1. 2.]\n",
            " [3. 4.]\n",
            " [5. 6.]], shape=(3, 2), dtype=float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The shape of this matrix is 3, 2, 5\n",
        "data = [x for x in range(30)]\n",
        "_data = [data[x:x+5] for x in list(range(0, 30, 5))]\n",
        "Rank3_Tensor = tf.constant([[_data[0], _data[1]],\n",
        "                            [_data[2], _data[3]],\n",
        "                            [_data[4], _data[5]]])\n",
        "Rank3_Tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j3czWhpoOle",
        "outputId": "efcc7cbc-2054-4c8b-ec0a-580183446434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2, 5), dtype=int32, numpy=\n",
              "array([[[ 0,  1,  2,  3,  4],\n",
              "        [ 5,  6,  7,  8,  9]],\n",
              "\n",
              "       [[10, 11, 12, 13, 14],\n",
              "        [15, 16, 17, 18, 19]],\n",
              "\n",
              "       [[20, 21, 22, 23, 24],\n",
              "        [25, 26, 27, 28, 29]]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ***matrix operations***"
      ],
      "metadata": {
        "id": "1sgR75Mue48P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]], dtype=tf.int32)\n",
        "b = tf.ones([2, 2], dtype=tf.int32)\n",
        "\n",
        "print('\\n')\n",
        "print(tf.add(a, b))\n",
        "print('\\n')\n",
        "print(tf.multiply(a, b))\n",
        "print('\\n')\n",
        "print(tf.matmul(a, b))\n",
        "print('\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u_H9hiloOgl",
        "outputId": "fba5189d-6069-4447-be92-5336b5bc0569"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "tf.Tensor(\n",
            "[[2 3]\n",
            " [4 5]], shape=(2, 2), dtype=int32)\n",
            "\n",
            "\n",
            "tf.Tensor(\n",
            "[[1 2]\n",
            " [3 4]], shape=(2, 2), dtype=int32)\n",
            "\n",
            "\n",
            "tf.Tensor(\n",
            "[[3 3]\n",
            " [7 7]], shape=(2, 2), dtype=int32)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = tf.constant([[4.0, 5.0], [10.0, 1.0]])\n",
        "print('\\t 1. Find the largest value: ', tf.reduce_max(c)) \n",
        "print('\\t 2. Find the index of the largest value: ', tf.argmax(c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkGUtkAToObo",
        "outputId": "cd3b7918-1ddf-429d-e744-36876b0b92dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\t 1. Find the largest value:  tf.Tensor(10.0, shape=(), dtype=float32)\n",
            "\t 2. Find the index of the largest value:  tf.Tensor([1 0], shape=(2,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# library setting\n",
        "import numpy as np\n",
        "\n",
        "# Compute the softmax\n",
        "'''\n",
        "    - 기본적인 softmax 연산은 아래와 같이 tf 라이브러리를 통해 매우 쉽에 도출할 수 있다.\n",
        "    - 더불어, softmax는 기본적으로 sigmoid의 결과값을 normalization한 것으로, 명확한 이해를 위해서는 sigmoid의 개념을 이해해야 하며, sigmoid를 적극적으로 활요하는 알고리즘 중 하나인\n",
        "    Logistic Regression을 통해 해당 경과를 코드로 설명해보겠다.\n",
        "\n",
        "        ** 변수설명 **\n",
        "\n",
        "        1. cost : 기존의 linear regression의 cost 그래로를 sigmoid function을 적용하여 계산하면 시작점에 따라 local minimum 값이 달라지고 따라서 박복학습에 따라  \n",
        "        global minimum 값에 수렴할 수 없게 됩니다. 이런 문제를 해결하기 위해, y = 0인 경우와 1인 경우에 대하여 log를 취한 2개의 cost function을 취하게 되고 이를 하나로 결합하기 위해\n",
        "        (1 - y)을 추가한 것 들 값들의 평균을 구하게 됩니다.\n",
        "\n",
        "        2. lr : learning rate \n",
        "\n",
        "        3. W : Weight\n",
        "\n",
        "        4. b : bias\n",
        "\n",
        "'''\n",
        "\n",
        "# logistic regression with sigmoid function\n",
        "'''\n",
        "    cost = tf.reduce_mean(-tf.reduce_sum(Y*tf.log(hypothesis) + (1-Y)*tf.log(1-hypothesis)))\n",
        "    lr = tf.Variable(0.1)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(a)\n",
        "    train = optimizer.minimize(cost)\n",
        "'''\n",
        "# 이번에는 실제 Logistic regression을 만들어보겠습니다.\n",
        "\n",
        "#   2. define dataset\n",
        "x_train = np.array([[1., 1.],\n",
        "                    [1., 2.],\n",
        "                    [2., 1.],\n",
        "                    [3., 2.],\n",
        "                    [3., 3.],\n",
        "                    [2., 3.]], dtype=np.float32)\n",
        "y_train = np.array([[0.],\n",
        "                    [0.],\n",
        "                    [0.],\n",
        "                    [1.],\n",
        "                    [1.],\n",
        "                    [1.]], dtype=np.float32)\n",
        "\n",
        "x_test = np.array([[3., 0.],\n",
        "                   [4., 1.]], dtype=np.float32)\n",
        "y_test = np.array([[0.],\n",
        "                   [1.]], dtype=np.float32)\n",
        "\n",
        "\n",
        "#   2. Initializing Weight\n",
        "tf.random.set_seed(1)\n",
        "W = tf.Variable(tf.random.normal([2, 1], mean=0.0))\n",
        "b = tf.Variable(tf.random.normal([1], mean=0.0))\n",
        "\n",
        "#   3. Train the model\n",
        "learning_rate = 0.01\n",
        "\n",
        "def predict(X):\n",
        "    z = tf.matmul(X, W) + b\n",
        "    hypothesis = 1 / (1 + tf.exp(-z))\n",
        "    return hypothesis\n",
        "\n",
        "print('\\n')\n",
        "for i in range(2*10**3+1):\n",
        "    '''\n",
        "        GradientTape는 지정한 모델의 예측력에 대한 미분값을 저장합니다.\n",
        "        일반적으로는 Optimizer와 함께 사용할테지만, 이번 예제는 cost값을 직접 계산해볼 것임으로 cost값과 및 weight, bias간의 각 미분값을 구하여 학습에 따른 변화 양상을 살펴보겠습니다.\n",
        "    '''\n",
        "    with tf.GradientTape() as tape:\n",
        "        hypothesis = predict(x_train)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(y_train*tf.math.log(hypothesis) + (1 - y_train)*tf.math.log(1 - hypothesis)))\n",
        "        W_grad, b_grad = tape.gradient(cost, [W, b])\n",
        "\n",
        "        W.assign_sub(learning_rate * W_grad)\n",
        "        b.assign_sub(learning_rate * b_grad)\n",
        "\n",
        "    if i % 400 == 0:\n",
        "        print('\\t >>> Bstch: {:<3} Weights: [{:}, {:}] \\n\\t\\t Bias: {:}, cost: {:} \\n'.format(i, W.numpy()[0][0], W.numpy()[1][0], b.numpy(), cost.numpy()))        "
      ],
      "metadata": {
        "id": "JVhbz0wUoOX_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b64152a-371e-4834-da54-45411a395d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\t >>> Bstch: 0   Weights: [-1.1069523096084595, 1.531254768371582] \n",
            "\t\t Bias: [0.3891292], cost: 5.080659866333008 \n",
            "\n",
            "\t >>> Bstch: 400 Weights: [0.44699472188949585, 0.9109866619110107] \n",
            "\t\t Bias: [-2.3015914], cost: 2.2290866374969482 \n",
            "\n",
            "\t >>> Bstch: 800 Weights: [1.0215262174606323, 1.1129134893417358] \n",
            "\t\t Bias: [-3.9197462], cost: 1.4660948514938354 \n",
            "\n",
            "\t >>> Bstch: 1200 Weights: [1.3341188430786133, 1.3564225435256958] \n",
            "\t\t Bias: [-5.06228], cost: 1.0973552465438843 \n",
            "\n",
            "\t >>> Bstch: 1600 Weights: [1.558021903038025, 1.564527988433838] \n",
            "\t\t Bias: [-5.9444833], cost: 0.8783029913902283 \n",
            "\n",
            "\t >>> Bstch: 2000 Weights: [1.7369108200073242, 1.7390990257263184] \n",
            "\t\t Bias: [-6.663596], cost: 0.7328943014144897 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 2>***연이어 우리는 Softmax를 이해하기 위해, 먼저 Sigmoid 기반의 Multinomial classification을 정의하고, 해당 배경을 가지고 Softmax Regression을 이해해 볼 것입니다.***</font>"
      ],
      "metadata": {
        "id": "KtyYPxdCuWDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.constant([['WA1', 'WA2', 'WA3'],\n",
        "             ['WB1', 'WB2', 'WB3'],\n",
        "             ['WC1', 'WC2', 'WC3']])\n",
        "b = tf.constant([['x1'],\n",
        "                 ['x2'],\n",
        "                 ['x3']])\n",
        "c = tf.constant([['WA1x1 + WA2x2 + WA3x3'],\n",
        "                 ['WB1x1 + WB2x2 + WB3x3'],\n",
        "                 ['WC1x1 + WC2x2 + WC3x3']])\n",
        "\n",
        "print('\\n')\n",
        "print('a: ', a)\n",
        "print('\\n')\n",
        "print('b: ', b)\n",
        "print('\\n')\n",
        "print('c: ', c)\n",
        "print('\\n')"
      ],
      "metadata": {
        "id": "e1zplOljhjUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3ab822-e219-4b14-8c99-ed164bab023c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "a:  tf.Tensor(\n",
            "[[b'WA1' b'WA2' b'WA3']\n",
            " [b'WB1' b'WB2' b'WB3']\n",
            " [b'WC1' b'WC2' b'WC3']], shape=(3, 3), dtype=string)\n",
            "\n",
            "\n",
            "b:  tf.Tensor(\n",
            "[[b'x1']\n",
            " [b'x2']\n",
            " [b'x3']], shape=(3, 1), dtype=string)\n",
            "\n",
            "\n",
            "c:  tf.Tensor(\n",
            "[[b'WA1x1 + WA2x2 + WA3x3']\n",
            " [b'WB1x1 + WB2x2 + WB3x3']\n",
            " [b'WC1x1 + WC2x2 + WC3x3']], shape=(3, 1), dtype=string)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 2>***Multinomial classification을 Sifgmoid를 통해 정의 하면 위에서 정의한 각 a, b, c에 대하여 a * b = c인 것과 같습니다. 각, $z = \\sum_{i=1}^{3} (W_i*X_i)$는 hypothesis인 Sigmoid를 거쳐 $\\bar{Y}$를 얻게 됩니다. 여기서 중요한 것은 각 $\\bar{Y}$를 얻게 된다는 것입니다. 즉, 독립된 classification을 수행하게 된다는 것입니다.***</font>"
      ],
      "metadata": {
        "id": "kyOcMbnXwDIW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 2>***그런데 이렇게 독립된 classification을 수행하게 되면 상대적으로 큰 값을 구분해낼 수야 있지만, 그 값의 차이가 때론 아주 크게, 때론 아주 작기 때문에 그 차이의 정도를 체감하기는 어려울 것입니다. 그러나 Softmax의 경우 다음과 같은 문제를 해결합니다.***</font>\n",
        "\n",
        "<font size = 2>***softmax는 scores에 대해 $\bS(y_i) = \\frac{\\mathrm{e}^{y_i}}{\\sum_{j} \\mathrm{e}^{y_j}}$와 같이 정의 되며 probabilities를 반환합니다.***</font>"
      ],
      "metadata": {
        "id": "e_14w1kKFNGF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size = 2>***여기까지를 통해 우리는 1번의 학습 과정을 마쳐야 합니다. 이제 반복학습을 통해 모델의 성능을 Update 해가야 하는데, 우리는 단순히 학습을 반복적으로 이루는 것이 아니라 모델을 평가해야 (by cost function) 하고 성능을 개선해가야 (by gradient descent) 합니다***</font>"
      ],
      "metadata": {
        "id": "q8HsBk8YKhB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cost function\n",
        "\n",
        "<font size = 2>***Cross-Entropy $H(p,q) = -\\sum_{i}p(x)*log(q(x))$를 통해 모델을 평가하고 해당 loss의 최소값을 도출하는 것이 모델의 목적입니다. 여기서 p는 실제 True probability distribution (One-Hot)을 나타내고 q는 model's predicted probability distribution을 나타냅니다.***</font>"
      ],
      "metadata": {
        "id": "7dkIIS6F7D5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PNHyalWkFMpx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5ygdwsmVhjST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8t8YIqo7hjP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MCxTLyidhjNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZmJQIgIkf_dE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "R5Vk-KAvf_a8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p0K2gyBQf_Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "21xbJXHQf_Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "mVe4lfoFf_Up"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WDt_nRjdf_Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sOQkqnLpf_Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "X6xQTW6Ef_OS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xdo__pb8f_L1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "UjSwQDSLf_Jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\"A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\" (https://www.tensorflow.org/guide/tensor)\n",
        "\n",
        "It should't surprise you that tensors are a fundemental apsect of TensorFlow. They are the main objects that are passed around and manipluated throughout the program. Each tensor represents a partialy defined computation that will eventually produce a value. TensorFlow programs work by building a graph of Tensor objects that details how tensors are related. Running different parts of the graph allow results to be generated.\n",
        "\n",
        "Each tensor has a data type and a shape. \n",
        "\n",
        "**Data Types Include**: float32, int32, string and others.\n",
        "\n",
        "**Shape**: Represents the dimension of data.\n",
        "\n",
        "Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc.\n",
        "\n",
        "In the next sections we will discuss some different properties of tensors. This is to make you more familiar with how tensorflow represnts data and how you can manipulate this data.\n"
      ],
      "metadata": {
        "id": "8imUFm5iY68d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LTgDuVIv6PyD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}